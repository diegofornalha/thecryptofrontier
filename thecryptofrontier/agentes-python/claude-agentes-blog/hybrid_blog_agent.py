import json
import os
from typing import Dict, Any, List
from datetime import datetime
import asyncio
import aiohttp

from llm_interface import LLMInterface, ClaudeLLM, GeminiLLM

class HybridBlogAgent:
    """
    Agente de blog h√≠brido que pode usar Claude CLI ou Gemini (via simula√ß√£o).
    """
    
    def __init__(self, llm: LLMInterface):
        self.llm = llm
        self.strapi_url = os.getenv('STRAPI_URL', 'http://localhost:1338')
        self.strapi_token = os.getenv('STRAPI_API_TOKEN')
        self.tasks_dir = "/home/strapi/thecryptofrontier/claude-agents/tasks" # Manter para compatibilidade ou remover se n√£o for mais necess√°rio
        os.makedirs(self.tasks_dir, exist_ok=True)
    
    def create_llm_prompt(self, task_type: str, context: Dict[str, Any]) -> str:
        """Cria o prompt para o LLM baseado no tipo de tarefa e contexto."""
        
        # Template baseado no tipo de tarefa
        templates = {
            "create_post": f"""
# Tarefa: Criar Post de Blog

## Contexto
- T√≥pico: {context.get('topic')}
- Palavras-chave: {', '.join(context.get('keywords', []))}
- Estilo: {context.get('style', 'profissional e acess√≠vel')}

## Instru√ß√µes

Por favor, crie um post de blog completo com:

1. **T√≠tulo** (m√°ximo 60 caracteres, otimizado para SEO)
2. **Slug** (URL amig√°vel)
3. **Meta Descri√ß√£o** (150-160 caracteres)
4. **Conte√∫do** (1500-2000 palavras em Markdown)
   - Introdu√ß√£o engajante
   - 5-7 se√ß√µes com subt√≠tulos (H2, H3)
   - Exemplos pr√°ticos
   - Estat√≠sticas relevantes
   - Conclus√£o com CTA
5. **Tags** (5-7 tags relevantes)
6. **Categorias** (1-2 categorias)

## Formato de Sa√≠da

Por favor, forne√ßa a sa√≠da em formato JSON, conforme o exemplo abaixo:

```json
{{
    "title": "...",
    "slug": "...",
    "excerpt": "...",
    "content": "...",
    "seo": {{
        "metaTitle": "...",
        "metaDescription": "..."
    }},
    "tags": ["..."],
    "categories": ["..."]
}}
```
""",
            
            "enhance_content": f"""
# Tarefa: Melhorar Conte√∫do Existente

## Conte√∫do Original
{context.get('content')}

## Instru√ß√µes

Melhore o conte√∫do:
1. Clareza e fluidez
2. Otimiza√ß√£o SEO
3. Engagement do leitor
4. Corre√ß√£o gramatical
5. Adicione CTAs relevantes

Mantenha o tom: {context.get('tone', 'profissional')}

## Formato de Sa√≠da

Por favor, forne√ßa o conte√∫do melhorado em formato Markdown.
""",
            
            "research_topic": f"""
# Tarefa: Pesquisar T√≥pico

## T√≥pico: {context.get('topic')}

## √Åreas de Pesquisa
1. Tend√™ncias atuais
2. Estat√≠sticas relevantes (2024-2025)
3. Cases de sucesso
4. Melhores pr√°ticas
5. Insights √∫nicos

## Output
Compile um relat√≥rio estruturado com:
- Resumo executivo
- Principais descobertas
- Dados e estat√≠sticas
- Recomenda√ß√µes
- Fontes (quando poss√≠vel)

Por favor, forne√ßa o relat√≥rio em formato Markdown.
"""
        }
        
        return templates.get(task_type, "Tarefa n√£o definida")
    
    async def execute_llm_task(self, task_type: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Executa a tarefa usando o LLM configurado.
        """
        prompt = self.create_llm_prompt(task_type, context)
        print(f"ü§ñ Enviando prompt para o LLM ({self.llm.__class__.__name__})...")
        llm_output = self.llm.generate_content(prompt)
        print(f"‚úÖ Resposta do LLM recebida.")
        return llm_output
    
    async def process_llm_output_and_publish(self, llm_output: Dict[str, Any]) -> Dict[str, Any]:
        """Processa o output do LLM e publica no Strapi"""
        
        # Assumimos que o llm_output j√° √© um dicion√°rio JSON v√°lido
        # ou que o m√©todo generate_content do LLM j√° fez o parsing.
        post_data = llm_output
            
        # Publica no Strapi
        return await self.publish_to_strapi(post_data)
            
    async def publish_to_strapi(self, post_data: Dict[str, Any]) -> Dict[str, Any]:
        """Publica post no Strapi"""
        
        headers = {
            'Authorization': f'Bearer {self.strapi_token}',
            'Content-Type': 'application/json'
        }
        
        strapi_data = {
            'data': {
                'title': post_data.get('title'),
                'slug': post_data.get('slug'),
                'content': post_data.get('content'),
                'excerpt': post_data.get('excerpt'),
                'seo': post_data.get('seo'),
                'tags': post_data.get('tags', []),
                'categories': post_data.get('categories', []),
                'status': 'published',
                'publishedAt': datetime.now().isoformat()
            }
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.strapi_url}/api/posts",
                headers=headers,
                json=strapi_data
            ) as response:
                if response.status in [200, 201]:
                    result = await response.json()
                    print(f"‚úÖ Post publicado: {result['data']['id']}")
                    return result
                else:
                    print(f"‚ùå Erro ao publicar: {response.status}")
                    print(f"Detalhes do erro: {await response.text()}")
                    return None

# Exemplo de uso
async def create_blog_post_with_llm(llm_choice: str, topic: str, keywords: List[str]):
    """Cria tarefa para o LLM processar e publica no Strapi."""
    
    llm_instance: LLMInterface
    if llm_choice.lower() == 'claude':
        llm_instance = ClaudeLLM()
    elif llm_choice.lower() == 'gemini':
        llm_instance = GeminiLLM()
    else:
        raise ValueError("Escolha de LLM inv√°lida. Use 'claude' ou 'gemini'.")

    agent = HybridBlogAgent(llm_instance)
    
    context = {
        'topic': topic,
        'keywords': keywords,
        'style': 'profissional mas acess√≠vel'
    }
    
    llm_output = await agent.execute_llm_task('create_post', context)
    
    if llm_output:
        await agent.process_llm_output_and_publish(llm_output)
    else:
        print("‚ùå Nenhuma sa√≠da do LLM para processar.")

if __name__ == "__main__":
    # Exemplo: Criar tarefa usando Gemini (simulado)
    asyncio.run(create_blog_post_with_llm(
        llm_choice="gemini",
        topic="O futuro das criptomoedas em 2025",
        keywords=["criptomoedas", "blockchain", "investimento", "tend√™ncias"]
    ))

    # Exemplo: Criar tarefa usando Claude (simulado)
    # asyncio.run(create_blog_post_with_llm(
    #     llm_choice="claude",
    #     topic="Impacto da regulamenta√ß√£o de cripto no Brasil",
    #     keywords=["regulamenta√ß√£o cripto", "Brasil", "legisla√ß√£o", "mercado"]
    # ))